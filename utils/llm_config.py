llm_config = {
    "model": "vicuna",
    "api_type": "ollama",
    "base_url": "http://localhost:11434/v1",
    "api_key": None,
    "temperature": 0.7,
}